# ============================================
# H∆Ø·ªöNG D·∫™N DEPLOY BIG DATA PIPELINE L√äN GOOGLE KUBERNETES ENGINE (GKE)
# D√†nh cho teammate test ri√™ng c√°c service: Kafka, Spark Streaming, Cassandra, Elasticsearch, Streamlit
# ============================================

# ============================================
# PH·∫¶N 1: SETUP M√îI TR∆Ø·ªúNG (Ch·ªâ l√†m 1 l·∫ßn)
# ============================================

# 1. C√†i ƒë·∫∑t Google Cloud SDK tr√™n Ubuntu WSL
curl https://sdk.cloud.google.com | bash
exec -l $SHELL

# 2. ƒêƒÉng nh·∫≠p Google Cloud
gcloud init
# ‚Üí Ch·ªçn account Google c·ªßa b·∫°n
# ‚Üí T·∫°o ho·∫∑c ch·ªçn project (v√≠ d·ª•: my-bigdata-project-123)
# ‚Üí Ch·ªçn region m·∫∑c ƒë·ªãnh: asia-northeast1

# 3. C√†i kubectl v√† plugin GKE
gcloud components install kubectl
gcloud components install gke-gcloud-auth-plugin

# 4. B·∫≠t c√°c API c·∫ßn thi·∫øt
gcloud services enable container.googleapis.com
gcloud services enable artifactregistry.googleapis.com
gcloud services enable cloudbuild.googleapis.com

# ============================================
# PH·∫¶N 2: T·∫†O GKE CLUSTER (M·ªói ng∆∞·ªùi 1 cluster ri√™ng)
# ============================================

# 5. T·∫°o cluster GKE c·ªßa ri√™ng b·∫°n
# Thay [YOUR_NAME] b·∫±ng t√™n c·ªßa b·∫°n (v√≠ d·ª•: tung-cluster, dat-cluster)
gcloud container clusters create [YOUR_NAME]-cluster \
  --zone asia-northeast1-c \
  --num-nodes 3 \
  --machine-type e2-standard-4 \
  --disk-size 50 \
  --enable-autoscaling \
  --min-nodes 1 \
  --max-nodes 5 \
  --enable-autorepair \
  --enable-autoupgrade

# 6. K·∫øt n·ªëi kubectl v·ªõi cluster
gcloud container clusters get-credentials [YOUR_NAME]-cluster \
  --zone asia-northeast1-c \
  --project [YOUR_PROJECT_ID]

# 7. Ki·ªÉm tra k·∫øt n·ªëi
kubectl config current-context
kubectl get nodes

# ============================================
# PH·∫¶N 3: DEPLOY KAFKA KRAFT (Message Broker)
# ============================================

# 8. T·∫°o namespace cho Kafka
kubectl create namespace kafka

# 9. C√†i ƒë·∫∑t Strimzi Operator
kubectl create -f 'https://strimzi.io/install/latest?namespace=kafka' -n kafka

# 10. Ch·ªù Operator kh·ªüi ƒë·ªông
kubectl get pods -n kafka -w
# Ch·ªù ƒë·∫øn khi strimzi-cluster-operator Running

# 11. Deploy Kafka KRaft cluster
kubectl apply -f kafka-kraft.yaml

# 12. Ch·ªù Kafka cluster kh·ªüi ƒë·ªông (5-10 ph√∫t)
kubectl get pods -n kafka -w
# Ch·ªù ƒë·∫øn khi my-cluster-dual-role-0, my-cluster-dual-role-1, my-cluster-dual-role-2 Running

# 13. L·∫•y EXTERNAL-IP c·ªßa Kafka
kubectl get svc -n kafka | grep external-bootstrap
# L∆∞u l·∫°i EXTERNAL-IP (v√≠ d·ª•: 34.180.65.245)

# 14. Test Kafka v·ªõi Producer/Consumer
export KAFKA_EXTERNAL_IP=[IP_T·ª™_B∆Ø·ªöC_13]
export KAFKA_BOOTSTRAP_SERVERS=$KAFKA_EXTERNAL_IP:9094

# T·∫°o venv v√† c√†i th∆∞ vi·ªán
cd kafka-producer
python3 -m venv venv
source venv/bin/activate
pip install kafka-python

# Ch·∫°y Producer
cd src
python producer.py

# M·ªü terminal m·ªõi, ch·∫°y Consumer
cd kafka-producer
source venv/bin/activate
export KAFKA_EXTERNAL_IP=[IP_T·ª™_B∆Ø·ªöC_13]
cd src
python consumer.py

# ============================================
# PH·∫¶N 4: BUILD & PUSH DOCKER IMAGES L√äN GOOGLE CLOUD
# ============================================

# 15. T·∫°o Artifact Registry
gcloud artifacts repositories create my-repo \
  --repository-format=docker \
  --location=asia-northeast1 \
  --description="Docker repository for Big Data Pipeline"

# 16. Build v√† Push images b·∫±ng Cloud Build
cd /path/to/big_data_pipeline
./scripts/push-to-gke.sh

# Ho·∫∑c build t·ª´ng service ri√™ng:
cd kafka-producer
gcloud builds submit --config=cloudbuild.yaml --substitutions=_PROJECT_ID=[YOUR_PROJECT_ID] .

cd ../spark-streaming
gcloud builds submit --config=cloudbuild.yaml --substitutions=_PROJECT_ID=[YOUR_PROJECT_ID] .

cd ../streamlit-dashboard
gcloud builds submit --config=cloudbuild.yaml --substitutions=_PROJECT_ID=[YOUR_PROJECT_ID] .

# 17. Ki·ªÉm tra images ƒë√£ push th√†nh c√¥ng
gcloud artifacts docker images list \
  asia-northeast1-docker.pkg.dev/[YOUR_PROJECT_ID]/my-repo \
  --include-tags

# ============================================
# PH·∫¶N 5: DEPLOY H·∫† T·∫¶NG (Elasticsearch, Cassandra)
# ============================================

# 18. T·∫°o namespace cho big data pipeline
kubectl create namespace big-data-pipeline

# 19. Deploy Elasticsearch
kubectl apply -f k8s/03-elasticsearch.yaml

# 20. Deploy Kibana (optional)
kubectl apply -f k8s/04-kibana.yaml

# 21. Deploy Cassandra
kubectl apply -f k8s/09-cassandra.yaml

# 22. Ch·ªù c√°c pods kh·ªüi ƒë·ªông
kubectl get pods -n big-data-pipeline -w
# Ch·ªù ƒë·∫øn khi elasticsearch-0, cassandra-0 Running

# 23. Ki·ªÉm tra Cassandra ƒë√£ init schema ch∆∞a
kubectl logs -n big-data-pipeline job/cassandra-schema-init

# 24. Test k·∫øt n·ªëi Cassandra
kubectl exec -it cassandra-0 -n big-data-pipeline -- cqlsh
# Trong cqlsh:
DESCRIBE KEYSPACES;
USE bigdata_pipeline;
DESCRIBE TABLES;
exit

# ============================================
# PH·∫¶N 6: DEPLOY ·ª®NG D·ª§NG (Producer, Spark, Streamlit)
# ============================================

# 25. Deploy Kafka Producer
kubectl apply -f k8s/05-kafka-producer.yaml

# 26. Deploy Spark Streaming
kubectl apply -f k8s/06-spark-streaming.yaml

# 27. Deploy Streamlit Dashboard
kubectl apply -f k8s/07-streamlit.yaml

# 28. Ki·ªÉm tra t·∫•t c·∫£ pods
kubectl get pods -n big-data-pipeline
kubectl get pods -n kafka

# 29. Xem logs c·ªßa t·ª´ng service
kubectl logs -n big-data-pipeline deployment/kafka-producer --follow
kubectl logs -n big-data-pipeline deployment/spark-streaming --follow
kubectl logs -n big-data-pipeline deployment/streamlit --follow

# ============================================
# PH·∫¶N 7: TRUY C·∫¨P C√ÅC SERVICE
# ============================================

# 30. L·∫•y EXTERNAL-IP c·ªßa Streamlit Dashboard
kubectl get svc streamlit -n big-data-pipeline
# Truy c·∫≠p: http://[EXTERNAL-IP]:8501

# 31. L·∫•y EXTERNAL-IP c·ªßa Kibana (n·∫øu deploy)
kubectl get svc kibana -n big-data-pipeline
# Truy c·∫≠p: http://[EXTERNAL-IP]:5601

# 32. Port-forward Elasticsearch (n·∫øu mu·ªën truy c·∫≠p t·ª´ local)
kubectl port-forward -n big-data-pipeline svc/elasticsearch 9200:9200
# Truy c·∫≠p: http://localhost:9200

# ============================================
# PH·∫¶N 8: MONITORING & DEBUG
# ============================================

# 33. Xem t·∫•t c·∫£ services
kubectl get svc --all-namespaces

# 34. Xem events c·ªßa namespace
kubectl get events -n big-data-pipeline --sort-by='.lastTimestamp'

# 35. Describe pod n·∫øu c√≥ l·ªói
kubectl describe pod [POD_NAME] -n big-data-pipeline

# 36. Exec v√†o container ƒë·ªÉ debug
kubectl exec -it [POD_NAME] -n big-data-pipeline -- /bin/bash

# 37. Xem logs real-time
kubectl logs -n big-data-pipeline [POD_NAME] --follow --tail=100

# 38. Xem resource usage
kubectl top nodes
kubectl top pods -n big-data-pipeline

# ============================================
# PH·∫¶N 9: SCALE & AUTOSCALING
# ============================================

# 39. Scale s·ªë replicas c·ªßa service
kubectl scale deployment kafka-producer --replicas=3 -n big-data-pipeline

# 40. Enable autoscaling cho cluster
gcloud container node-pools update default-pool \
  --cluster=[YOUR_NAME]-cluster \
  --zone=asia-northeast1-c \
  --enable-autoscaling \
  --min-nodes=1 \
  --max-nodes=5

# 41. Enable Horizontal Pod Autoscaler (HPA)
kubectl autoscale deployment spark-streaming \
  --cpu-percent=80 \
  --min=1 \
  --max=5 \
  -n big-data-pipeline

# ============================================
# PH·∫¶N 10: CLEANUP (Khi test xong)
# ============================================

# 42. X√≥a to√†n b·ªô deployments trong namespace
kubectl delete namespace big-data-pipeline
kubectl delete namespace kafka

# 43. X√≥a cluster (ti·∫øt ki·ªám chi ph√≠)
gcloud container clusters delete [YOUR_NAME]-cluster \
  --zone asia-northeast1-c \
  --quiet

# 44. X√≥a Artifact Registry repository
gcloud artifacts repositories delete my-repo \
  --location=asia-northeast1 \
  --quiet

# ============================================
# PH·∫¶N 11: TROUBLESHOOTING COMMON ISSUES
# ============================================

# L·ªói ImagePullBackOff:
# ‚Üí Ki·ªÉm tra image ƒë√£ push l√™n Artifact Registry ch∆∞a
# ‚Üí Ki·ªÉm tra imagePullPolicy trong YAML

# L·ªói CrashLoopBackOff:
# ‚Üí Xem logs: kubectl logs [POD_NAME] -n [NAMESPACE]
# ‚Üí Ki·ªÉm tra resources limits/requests
# ‚Üí Ki·ªÉm tra dependencies (Kafka, Cassandra c√≥ s·∫µn ch∆∞a)

# Pod Pending:
# ‚Üí Xem events: kubectl describe pod [POD_NAME]
# ‚Üí Ki·ªÉm tra node c√≥ ƒë·ªß resources kh√¥ng
# ‚Üí Scale cluster n·∫øu c·∫ßn

# Kh√¥ng k·∫øt n·ªëi ƒë∆∞·ª£c Kafka:
# ‚Üí Ki·ªÉm tra KAFKA_BOOTSTRAP_SERVERS ƒë√∫ng ch∆∞a
# ‚Üí Ki·ªÉm tra firewall rules cho port 9094
# ‚Üí Test: telnet [KAFKA_IP] 9094

# ============================================
# PH·∫¶N 12: TIPS & BEST PRACTICES
# ============================================

# 1. Lu√¥n d√πng namespace ri√™ng cho m·ªói m√¥i tr∆∞·ªùng (dev, staging, prod)
# 2. Set resource limits ƒë·ªÉ tr√°nh m·ªôt pod chi·∫øm h·∫øt t√†i nguy√™n
# 3. Enable monitoring v·ªõi Prometheus/Grafana
# 4. Backup d·ªØ li·ªáu Cassandra ƒë·ªãnh k·ª≥
# 5. D√πng ConfigMap/Secret cho configuration thay v√¨ hardcode
# 6. Tag images v·ªõi version c·ª• th·ªÉ thay v√¨ :latest
# 7. Test k·ªπ tr√™n local tr∆∞·ªõc khi deploy production
# 8. Monitor cost tr√™n Google Cloud Console
# 9. T·∫Øt cluster khi kh√¥ng d√πng ƒë·ªÉ ti·∫øt ki·ªám chi ph√≠
# 10. ƒê·ªçc logs th∆∞·ªùng xuy√™n ƒë·ªÉ ph√°t hi·ªán l·ªói s·ªõm

# ============================================
# K·∫æT TH√öC - CH√öC B·∫†N DEPLOY TH√ÄNH C√îNG! üöÄ
# ============================================
