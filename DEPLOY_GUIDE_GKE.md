DÆ°á»›i Ä‘Ã¢y lÃ  hÆ°á»›ng dáº«n cÃ´ Ä‘á»ng vÃ  thá»±c táº¿ Ä‘á»ƒ báº¡n Ä‘Æ°a vÃ o bÃ¡o cÃ¡o hoáº·c tÃ i liá»‡u hÆ°á»›ng dáº«n.

Pháº§n 1 lÃ  **Quy trÃ¬nh cáº­p nháº­t code (CI/CD thá»§ cÃ´ng)** khi báº¡n sá»­a file Python.
Pháº§n 2 lÃ  **Ká»‹ch báº£n Demo (Showcase)** Ä‘á»ƒ trÃ¬nh bÃ y káº¿t quáº£ cho cÃ¡c tháº§y cÃ´, chá»©ng minh há»‡ thá»‘ng hoáº¡t Ä‘á»™ng thá»±c táº¿.

-----

### PHáº¦N 1: QUY TRÃŒNH Cáº¬P NHáº¬T CODE (UPDATE PIPELINE)

Khi báº¡n sá»­a báº¥t ká»³ file `.py` nÃ o (vÃ­ dá»¥: sá»­a logic Spark, sá»­a giao diá»‡n Streamlit), báº¡n **Báº®T BUá»˜C** pháº£i thá»±c hiá»‡n 3 bÆ°á»›c: **Build -\> Push -\> Restart Deployment**.

**Biáº¿n mÃ´i trÆ°á»ng chung (Cháº¡y lá»‡nh nÃ y trÆ°á»›c má»—i láº§n lÃ m viá»‡c):**

```bash
export PROJECT_ID="robust-magpie-479807-f1"
export NAMESPACE="big-data-pipeline"
```

#### 1\. Náº¿u sá»­a Kafka Producer (`producer.py`)

DÃ¹ng khi báº¡n muá»‘n thay Ä‘á»•i tá»‘c Ä‘á»™ gá»­i tin, hoáº·c thay Ä‘á»•i dá»¯ liá»‡u Ä‘áº§u vÃ o.

```bash
# 1. Di chuyá»ƒn vÃ o thÆ° má»¥c code
cd kafka-producer

# 2. Build vÃ  Push image má»›i lÃªn Cloud (sá»­ dá»¥ng Cloud Build)
echo "ðŸ“¦ Building & Pushing Kafka Producer..."
gcloud builds submit --config=cloudbuild.yaml --substitutions=_PROJECT_ID=$PROJECT_ID .

# 3. Cáº­p nháº­t Deployment trÃªn Kubernetes (KÃ©o image má»›i vá»)
echo "ðŸ”„ Rolling Update Kafka Producer..."
kubectl rollout restart deployment kafka-producer -n $NAMESPACE

# 4. Kiá»ƒm tra logs Ä‘á»ƒ cháº¯c cháº¯n code má»›i cháº¡y á»•n
echo "ðŸ” Checking Logs..."
kubectl logs -l app=kafka-producer -n $NAMESPACE --follow
```

#### 2\. Náº¿u sá»­a Spark Streaming (`streaming_app.py`)

DÃ¹ng khi báº¡n sá»­a logic tÃ­nh toÃ¡n, aggregation, watermark, hoáº·c logic ghi vÃ o DB.

```bash
# 1. Di chuyá»ƒn vÃ o thÆ° má»¥c code
cd spark-streaming

# 2. Build vÃ  Push image má»›i
echo "ðŸ“¦ Building & Pushing Spark Streaming..."
gcloud builds submit --config=cloudbuild.yaml --substitutions=_PROJECT_ID=$PROJECT_ID .

# 3. Cáº­p nháº­t Deployment (Spark sáº½ dá»«ng xá»­ lÃ½ cÅ© vÃ  cháº¡y xá»­ lÃ½ má»›i)
echo "ðŸ”„ Rolling Update Spark Streaming..."
kubectl rollout restart deployment spark-streaming -n $NAMESPACE

# 4. Kiá»ƒm tra logs (Quan trá»ng: xem cÃ³ lá»—i logic khÃ´ng)
echo "ðŸ” Checking Logs..."
kubectl logs -l app=spark-streaming -n $NAMESPACE --follow
```

#### 3\. Náº¿u sá»­a Streamlit Dashboard (`app.py`)

DÃ¹ng khi báº¡n chá»‰nh sá»­a biá»ƒu Ä‘á»“, mÃ u sáº¯c, hoáº·c cÃ¡ch hiá»ƒn thá»‹ dá»¯ liá»‡u.

```bash
# 1. Di chuyá»ƒn vÃ o thÆ° má»¥c code
cd streamlit-dashboard

# 2. Build vÃ  Push image má»›i
echo "ðŸ“¦ Building & Pushing Streamlit..."
gcloud builds submit --config=cloudbuild.yaml --substitutions=_PROJECT_ID=$PROJECT_ID .

# 3. Cáº­p nháº­t Deployment
echo "ðŸ”„ Rolling Update Streamlit..."
kubectl rollout restart deployment streamlit -n $NAMESPACE

# 4. Láº¥y láº¡i Ä‘á»‹a chá»‰ IP (náº¿u cáº§n, thÆ°á»ng IP khÃ´ng Ä‘á»•i)
kubectl get svc streamlit -n $NAMESPACE
```

-----

### PHáº¦N 2: Káº¾T QUáº¢ Cáº¦N SHOW CHO GIÃO VIÃŠN (DEMO SCRIPT)

Khi bÃ¡o cÃ¡o Ä‘á»“ Ã¡n, báº¡n cáº§n chá»©ng minh Ä‘Æ°á»£c luá»“ng dá»¯ liá»‡u Ä‘i tá»« Ä‘áº§u Ä‘áº¿n cuá»‘i (**End-to-End Pipeline**). HÃ£y má»Ÿ sáºµn cÃ¡c cá»­a sá»• sau:

#### 1\. Show Háº¡ táº§ng (Chá»©ng minh há»‡ thá»‘ng Distributed)

Má»Ÿ má»™t terminal vÃ  cháº¡y lá»‡nh nÃ y Ä‘á»ƒ cho tháº¥y táº¥t cáº£ cÃ¡c thÃ nh pháº§n Ä‘ang cháº¡y trÃªn Kubernetes (Cluster).

  * **Lá»‡nh:** `kubectl get pods -n big-data-pipeline`
  * **Äiá»ƒm nháº¥n:**
      * Chá»‰ vÃ o **Kafka, Zookeeper** (Message Queue).
      * Chá»‰ vÃ o **Elasticsearch, Cassandra** (NoSQL Databases).
      * Chá»‰ vÃ o **Spark Streaming** (Processing Engine).
      * Tráº¡ng thÃ¡i táº¥t cáº£ pháº£i lÃ  **Running**.

#### 2\. Show Luá»“ng Dá»¯ liá»‡u Real-time (Logs)

ÄÃ¢y lÃ  pháº§n "ká»¹ thuáº­t" nháº¥t, chá»©ng minh Spark Ä‘ang xá»­ lÃ½ tá»«ng giÃ¢y.

  * **Lá»‡nh:** `kubectl logs -l app=spark-streaming -n big-data-pipeline --follow`
  * **Giáº£i thÃ­ch:** "ÄÃ¢y lÃ  logs cá»§a Spark Streaming. CÃ¡c tháº§y cÃ³ thá»ƒ tháº¥y nÃ³ Ä‘ang xá»­ lÃ½ theo tá»«ng Batch (lÃ´ dá»¯ liá»‡u). DÃ²ng `Batch ... completed` hiá»‡n ra liÃªn tá»¥c nghÄ©a lÃ  dá»¯ liá»‡u Ä‘ang cháº£y tá»« Kafka qua Spark vÃ  Ä‘Æ°á»£c ghi xuá»‘ng Database."

#### 3\. Show Káº¿t quáº£ Trá»±c quan (Streamlit Dashboard)

ÄÃ¢y lÃ  pháº§n quan trá»ng nháº¥t Ä‘á»ƒ ngÆ°á»i xem dá»… hÃ¬nh dung.

  * **Truy cáº­p:** TrÃ¬nh duyá»‡t web `http://[EXTERNAL-IP]:8501`
  * **Äiá»ƒm nháº¥n:**
      * Chá»‰ vÃ o cÃ¡c biá»ƒu Ä‘á»“ tá»± Ä‘á»™ng cáº­p nháº­t (náº¿u báº¡n Ä‘á»ƒ auto-refresh) hoáº·c báº¥m nÃºt refresh.
      * Giáº£i thÃ­ch dá»¯ liá»‡u nÃ y láº¥y tá»« **Elasticsearch/Cassandra**, nÆ¡i mÃ  Spark vá»«a ghi dá»¯ liá»‡u vÃ o.
      * **Quan trá»ng:** Náº¿u cÃ³ thá»ƒ, hÃ£y Ä‘á»ƒ Kafka Producer cháº¡y cháº­m láº¡i má»™t chÃºt Ä‘á»ƒ tháº§y cÃ´ tháº¥y sá»‘ lÆ°á»£ng events tÄƒng dáº§n trÃªn biá»ƒu Ä‘á»“ theo thá»i gian thá»±c.

#### 4\. (TÃ¹y chá»n) Show Dá»¯ liá»‡u Gá»‘c trong Database

Náº¿u tháº§y cÃ´ há»i sÃ¢u "Dá»¯ liá»‡u lÆ°u vÃ o database trÃ´ng nhÆ° tháº¿ nÃ o?", báº¡n dÃ¹ng lá»‡nh nÃ y:

  * **Cassandra:**
    ```bash
    kubectl exec -it cassandra-0 -n big-data-pipeline -- cqlsh -e "SELECT * FROM bigdata_pipeline.events LIMIT 5;"
    ```
  * **Giáº£i thÃ­ch:** "ÄÃ¢y lÃ  dá»¯ liá»‡u thÃ´ Ä‘Ã£ Ä‘Æ°á»£c chuáº©n hÃ³a vÃ  lÆ°u trá»¯ bá»n vá»¯ng trong Cassandra."

-----

### ðŸ“ TÃ³m táº¯t Ká»‹ch báº£n Demo:

1.  **Má»Ÿ Ä‘áº§u:** "Há»‡ thá»‘ng bao gá»“m cÃ¡c thÃ nh pháº§n..." -\> Show **Terminal `kubectl get pods`**.
2.  **Input:** "Kafka Producer Ä‘ang Ä‘á»c file CSV tá»« Google Cloud Storage vÃ  báº¯n vÃ o há»‡ thá»‘ng..." -\> (Optional: Show log Producer).
3.  **Process:** "Spark Streaming Ä‘á»c tá»« Kafka, tá»•ng há»£p dá»¯ liá»‡u..." -\> Show **Terminal Log Spark**.
4.  **Output:** "Káº¿t quáº£ cuá»‘i cÃ¹ng Ä‘Æ°á»£c hiá»ƒn thá»‹ táº¡i Ä‘Ã¢y..." -\> Show **Web Dashboard**.