# Big Data Analytics Pipeline on Kubernetes

A complete big data analytics and visualization pipeline deployed on Kubernetes. This project demonstrates a scalable, real-time data processing system using modern big data technologies.

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Kafka Producer  â”‚ â”€â”€â–º Generate sample data
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Kafka Cluster   â”‚ â”€â”€â–º Message broker for data streaming
â”‚   + Zookeeper   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Spark Streaming â”‚ â”€â”€â–º Real-time data processing
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”œâ”€â”€â–º Elasticsearch â”€â”€â–º Kibana + Streamlit
         â”‚
         â””â”€â”€â–º Cassandra â”€â”€â–º Time-series storage
```

## ğŸ“¦ Components

- **Kafka Producer**: Python-based data generator that produces sample e-commerce events
- **Kafka + Zookeeper**: Distributed streaming platform for data ingestion
- **Spark Streaming**: Real-time data processing engine for analytics
- **Elasticsearch**: Search and analytics engine for data storage
- **Cassandra**: NoSQL database for time-series data storage with TTL
- **Kibana**: Data visualization and exploration tool
- **Streamlit**: Custom Python dashboard for real-time analytics
- **Prometheus + Grafana**: Monitoring and alerting stack

## ğŸ“ Project Structure

```
big_data_pipeline/
â”œâ”€â”€ kafka-producer/          # Python Kafka producer
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â””â”€â”€ producer.py     # Data generation and streaming
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ spark-streaming/         # Spark Streaming application
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â””â”€â”€ streaming_app.py # Real-time processing logic
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ streamlit-dashboard/     # Custom visualization dashboard
â”‚   â”œâ”€â”€ app.py              # Streamlit dashboard application
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ cassandra/              # Cassandra database
â”‚   â”œâ”€â”€ init-schema.cql     # Database schema initialization
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ k8s/                    # Kubernetes manifests
â”‚   â”œâ”€â”€ 00-namespace.yaml
â”‚   â”œâ”€â”€ 01-zookeeper.yaml
â”‚   â”œâ”€â”€ 02-kafka.yaml
â”‚   â”œâ”€â”€ 03-elasticsearch.yaml
â”‚   â”œâ”€â”€ 04-kibana.yaml
â”‚   â”œâ”€â”€ 05-kafka-producer.yaml
â”‚   â”œâ”€â”€ 06-spark-streaming.yaml
â”‚   â”œâ”€â”€ 07-streamlit.yaml
â”‚   â”œâ”€â”€ 08-monitoring.yaml
â”‚   â””â”€â”€ 09-cassandra.yaml
â”œâ”€â”€ monitoring/             # Monitoring configuration
â”‚   â””â”€â”€ prometheus.yml
â”œâ”€â”€ scripts/               # Automation scripts
â”‚   â”œâ”€â”€ build-images.sh   # Build Docker images
â”‚   â”œâ”€â”€ deploy.sh         # Deploy to Kubernetes
â”‚   â”œâ”€â”€ cleanup.sh        # Clean up resources
â”‚   â””â”€â”€ status.sh         # Check deployment status
â”œâ”€â”€ docker-compose.yml     # Local development setup
â””â”€â”€ README.md
```

## ğŸš€ Quick Start

### Prerequisites

- Docker and Docker Compose
- Kubernetes cluster (Minikube, Kind, or cloud provider)
- kubectl configured
- Python 3.9+ (for local development)
- Shared virtual environment (see VENV_README.md)

### Local Development with Docker Compose

1. **Clone the repository**
   ```bash
   git clone https://github.com/Tung-it2-k67-hust/big_data_pipeline.git
   cd big_data_pipeline
   ```

2. **Start all services**
   ```bash
   docker-compose up -d
   ```

3. **Access the dashboards**
   - Kibana: http://localhost:5601
   - Streamlit: http://localhost:8501
   - Prometheus: http://localhost:9090
   - Grafana: http://localhost:3000 (admin/admin)
   - Cassandra CQL: `docker exec -it cassandra cqlsh`

4. **Stop services**
   ```bash
   docker-compose down
   ```

### Kubernetes Deployment (Local - Minikube/Kind)

1. **Build Docker images**
   ```bash
   ./scripts/build-images.sh
   ```

2. **Deploy to Kubernetes**
   ```bash
   ./scripts/deploy.sh
   ```

3. **Check deployment status**
   ```bash
   ./scripts/status.sh
   # or
   kubectl get pods -n big-data-pipeline
   ```

4. **Access services via NodePort**
   - Kibana: http://<node-ip>:30561
   - Streamlit: http://<node-ip>:30851
   - Prometheus: http://<node-ip>:30909
   - Grafana: http://<node-ip>:30300
   - Cassandra CQL: `kubectl exec -it cassandra-0 -n big-data-pipeline -- cqlsh`

5. **Clean up**
   ```bash
   ./scripts/cleanup.sh
   ```

### Google Kubernetes Engine (GKE) Deployment â˜ï¸

Äá»ƒ deploy lÃªn Google Cloud Platform (GKE), xem hÆ°á»›ng dáº«n chi tiáº¿t: **[ğŸ“– GKE_DEPLOYMENT_GUIDE.md](docs/GKE_DEPLOYMENT_GUIDE.md)**

**Quick Start cho GKE:**

1. **Thiáº¿t láº­p GKE cluster** (náº¿u chÆ°a cÃ³)
   ```bash
   ./scripts/gke-setup-cluster.sh YOUR_PROJECT_ID
   ```

2. **Build vÃ  push images lÃªn Google Container Registry**
   ```bash
   ./scripts/gke-build-push.sh YOUR_PROJECT_ID
   ```

3. **Cáº­p nháº­t image paths**
   ```bash
   ./scripts/gke-update-images.sh YOUR_PROJECT_ID
   ```

4. **Deploy lÃªn GKE**
   ```bash
   ./scripts/gke-deploy.sh
   ```

5. **Kiá»ƒm tra tráº¡ng thÃ¡i**
   ```bash
   ./scripts/gke-status.sh
   ```

6. **Dá»n dáº¹p**
   ```bash
   ./scripts/gke-cleanup.sh
   ```

## ğŸ”§ Configuration

### Environment Variables

#### Kafka Producer
- `KAFKA_BOOTSTRAP_SERVERS`: Kafka broker address (default: `kafka:9092`)
- `KAFKA_TOPIC`: Topic name for data streaming (default: `data-stream`)
- `PRODUCER_INTERVAL`: Interval between messages in seconds (default: `1`)

#### Spark Streaming
- `KAFKA_BOOTSTRAP_SERVERS`: Kafka broker address
- `KAFKA_TOPIC`: Topic to consume from
- `ELASTICSEARCH_NODES`: Elasticsearch cluster address
- `ELASTICSEARCH_INDEX`: Index for raw events (default: `events`)
- `ELASTICSEARCH_AGG_INDEX`: Index for aggregated data (default: `events-aggregated`)

#### Streamlit Dashboard
- `ELASTICSEARCH_HOST`: Elasticsearch host (default: `elasticsearch`)
- `ELASTICSEARCH_PORT`: Elasticsearch port (default: `9200`)

## ğŸ“Š Data Flow

1. **Data Generation**: Kafka producer generates sample e-commerce events (clicks, views, purchases, searches)
2. **Ingestion**: Events are published to Kafka topic `data-stream`
3. **Processing**: Spark Streaming consumes events, performs real-time aggregations
4. **Storage**: Processed data is stored in Elasticsearch indices
5. **Visualization**: Kibana and Streamlit provide interactive dashboards

### Sample Event Schema

```json
{
  "timestamp": "2024-01-01T12:00:00.000000",
  "user_id": 1234,
  "event_type": "purchase",
  "product_id": 567,
  "price": 99.99,
  "quantity": 2,
  "session_id": 54321,
  "region": "US",
  "device": "mobile"
}
```

## ğŸ“ˆ Monitoring

### Prometheus Metrics

Prometheus collects metrics from:
- Kafka brokers
- Elasticsearch cluster
- Kubernetes pods

Access Prometheus at http://localhost:30909 (K8s) or http://localhost:9090 (Docker Compose)

### Grafana Dashboards

Grafana provides visualization for:
- System metrics
- Application performance
- Resource utilization

Default credentials: `admin/admin`

## ğŸ› ï¸ Development

### Running Components Locally

First, activate the shared virtual environment:

```bash
# Windows
.\venv\Scripts\activate.ps1

# Linux/Mac
source venv/bin/activate
```

#### Kafka Producer
```bash
cd kafka-producer
python src/producer.py
```

#### Spark Streaming
```bash
cd spark-streaming
python src/streaming_app.py
```

#### Streamlit Dashboard
```bash
cd streamlit-dashboard
streamlit run app.py
```

## ğŸ§ª Testing

### Manual Testing

1. Check if Kafka is receiving messages:
   ```bash
   kubectl exec -it kafka-0 -n big-data-pipeline -- kafka-console-consumer \
     --bootstrap-server localhost:9092 \
     --topic data-stream \
     --from-beginning
   ```

2. Check Elasticsearch indices:
   ```bash
   curl http://localhost:9200/_cat/indices?v
   ```

3. Query data from Elasticsearch:
   ```bash
   curl http://localhost:9200/events/_search?pretty
   ```

## ğŸ” Troubleshooting

### Common Issues

1. **Pods not starting**: Check resource limits and availability
   ```bash
   kubectl describe pod <pod-name> -n big-data-pipeline
   ```

2. **Kafka connection issues**: Ensure Zookeeper is running and healthy
   ```bash
   kubectl logs kafka-0 -n big-data-pipeline
   ```

3. **Elasticsearch disk space**: Monitor disk usage
   ```bash
   curl http://localhost:9200/_cluster/health?pretty
   ```

4. **Spark Streaming errors**: Check logs
   ```bash
   kubectl logs deployment/spark-streaming -n big-data-pipeline
   ```

## ğŸ“ Customization

### Adding New Data Sources

1. Modify `kafka-producer/src/producer.py` to generate different data
2. Update schema in `spark-streaming/src/streaming_app.py`
3. Adjust dashboard visualizations in `streamlit-dashboard/app.py`

### Scaling

- **Kafka**: Increase replicas in `k8s/02-kafka.yaml`
- **Spark**: Adjust resources and replicas in `k8s/06-spark-streaming.yaml`
- **Elasticsearch**: Scale nodes in `k8s/03-elasticsearch.yaml`

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Submit a pull request

## ğŸ“„ License

This project is open source and available under the MIT License.

## ğŸ‘¥ Authors

- Tung-it2-k67-hust

## ğŸ™ Acknowledgments

- Apache Kafka
- Apache Spark
- Elastic Stack
- Streamlit
- Kubernetes Community
